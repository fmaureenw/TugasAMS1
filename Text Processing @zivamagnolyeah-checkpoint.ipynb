{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe154e3",
   "metadata": {},
   "source": [
    "# Tugas 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde28f95",
   "metadata": {},
   "source": [
    "Mata Kuliah : Analitika Media Sosial \n",
    "\n",
    "Dosen Pengampu : Abu Salam, M.Kom\n",
    "\n",
    "    Nama : Faustina Maureen Widjojo\n",
    "    NIM : A12.2020.06526\n",
    "    Kelompok : A12.6501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19344ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import tweepy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72feedc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_config():\n",
    "    \"\"\"\n",
    "    Fungsi utilitas untuk mengkonfigurasi konsumsi file API Twitter dengan‚ê£\n",
    "    ,!kunci yang disediakan.\n",
    "    \"\"\"\n",
    "    # Otentikasi dan akses menggunakan kunci:\n",
    "    auth = tweepy.OAuthHandler(\"UTmKjFfUAfYyvzjnLZ3mrOuK5\",\"WAjJirxv7rJUryxwqA6NHGDgXK4cDf1ksKaeAHKvIo8BqE7Ca8\")\n",
    "    auth.set_access_token(\"1168154143082471424-Xj4KIDpCRfspUEFNw9VvcThGtkGIzb\",\"EjQ2dx611F5Sa8Z8moY8HmoKRkpr0PZLItVgkPnih5B5n\")\n",
    "    # Kembalikan akses ke API:\n",
    "    api = tweepy.API(auth)\n",
    "    try:\n",
    "        api.verify_credentials()\n",
    "        print(\"Authentication OK\")\n",
    "    except:\n",
    "        print(\"Error during authentication\")\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbd64703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication OK\n"
     ]
    }
   ],
   "source": [
    "# buat extractor object\n",
    "extractor = twitter_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "839bfa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets terambil: 100.\n",
      "\n",
      "10 tweet teratas:\n",
      "\n",
      "@Abdul_Husnan https://t.co/XkT38Lt9Ba\n",
      "\n",
      "@yegeye_ filihan yang terbaik?\n",
      "\n",
      "jangan merokok yaa mending deketin aku aja. soalnya kan rokok membunuhmu. sedangkan aku? ya mencintaimu.\n",
      "\n",
      "sisi kekanakan dari orang dewasa bisa dilihat dari supir-supir truk yang saling klakson-klaksonan di bawah trowonga‚Ä¶ https://t.co/zrsLyM0gYJ\n",
      "\n",
      "@aldysuhanda_ kamu cok @_kevr ü§°\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = extractor.user_timeline(screen_name=\"zivamagnolyeah\", count=100)\n",
    "print(\"Tweets terambil: {}.\\n\".format(len(tweets)))\n",
    "\n",
    "\n",
    "print(\"10 tweet teratas:\\n\")\n",
    "for tweet in tweets[:5]:\n",
    "    print(tweet.text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6efaa683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Abdul_Husnan https://t.co/XkT38Lt9Ba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@yegeye_ filihan yang terbaik?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jangan merokok yaa mending deketin aku aja. so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sisi kekanakan dari orang dewasa bisa dilihat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@aldysuhanda_ kamu cok @_kevr ü§°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>siapapun ngelawak dong pake meme cepat berikan üëáüèª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@yurayunita yuk bareng-bareng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>selamat pagi\\njangan lupa sikat gigi\\nbiar mul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@seterahdeh gelap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@8123rio artis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets\n",
       "0              @Abdul_Husnan https://t.co/XkT38Lt9Ba\n",
       "1                     @yegeye_ filihan yang terbaik?\n",
       "2  jangan merokok yaa mending deketin aku aja. so...\n",
       "3  sisi kekanakan dari orang dewasa bisa dilihat ...\n",
       "4                    @aldysuhanda_ kamu cok @_kevr ü§°\n",
       "5  siapapun ngelawak dong pake meme cepat berikan üëáüèª\n",
       "6                      @yurayunita yuk bareng-bareng\n",
       "7  selamat pagi\\njangan lupa sikat gigi\\nbiar mul...\n",
       "8                                  @seterahdeh gelap\n",
       "9                                     @8123rio artis"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Kita dapat membuat kerangka data sebagai berikut:\n",
    "dataset = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])\n",
    "\n",
    "# Kami membuat tampilan kerangka data:\n",
    "display(dataset.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e01101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec753a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Folding Result : \n",
      "\n",
      "0                @abdul_husnan https://t.co/xkt38lt9ba\n",
      "1                       @yegeye_ filihan yang terbaik?\n",
      "2    jangan merokok yaa mending deketin aku aja. so...\n",
      "3    sisi kekanakan dari orang dewasa bisa dilihat ...\n",
      "4                      @aldysuhanda_ kamu cok @_kevr ü§°\n",
      "Name: Tweets, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------ Case Folding --------\n",
    "# gunakan fungsi Series.str.lower() pada Pandas\n",
    "data['Tweets'] = data['Tweets'].str.lower()\n",
    "\n",
    "print('Case Folding Result : \\n')\n",
    "print(data['Tweets'].head(5))\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abd7c0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46a8f6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Result : \n",
      "\n",
      "0                                             [husnan]\n",
      "1                             [filihan, yang, terbaik]\n",
      "2    [jangan, merokok, yaa, mending, deketin, aku, ...\n",
      "3    [sisi, kekanakan, dari, orang, dewasa, bisa, d...\n",
      "4                                    [kamu, cok, kevr]\n",
      "Name: Text_tokens, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "import re #regex library\n",
    "\n",
    "# import word_tokenize & FreqDist from NLTK\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# ------ Tokenizing ---------\n",
    "\n",
    "def remove_tweet_special(text):\n",
    "    # remove tab, new line, ans back slice\n",
    "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "    # remove non ASCII (emoticon, chinese word, .etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # remove mention, link, hashtag\n",
    "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "    # remove incomplete URL\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "                \n",
    "data['Tweets'] = data['Tweets'].apply(remove_tweet_special)\n",
    "\n",
    "#remove number\n",
    "def remove_number(text):\n",
    "    return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "data['Tweets'] = data['Tweets'].apply(remove_number)\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "data['Tweets'] = data['Tweets'].apply(remove_punctuation)\n",
    "\n",
    "#remove whitespace leading & trailing\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "data['Tweets'] = data['Tweets'].apply(remove_whitespace_LT)\n",
    "\n",
    "#remove multiple whitespace into single whitespace\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "data['Tweets'] = data['Tweets'].apply(remove_whitespace_multiple)\n",
    "\n",
    "# remove single char\n",
    "def remove_singl_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "data['Tweets'] = data['Tweets'].apply(remove_singl_char)\n",
    "\n",
    "# NLTK word rokenize \n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "data['Text_tokens'] = data['Tweets'].apply(word_tokenize_wrapper)\n",
    "\n",
    "print('Tokenizing Result : \\n') \n",
    "print(data['Text_tokens'].head())\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9dba944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Tokens : \n",
      "\n",
      "0                                        [(husnan, 1)]\n",
      "1              [(filihan, 1), (yang, 1), (terbaik, 1)]\n",
      "2    [(aku, 2), (jangan, 1), (merokok, 1), (yaa, 1)...\n",
      "3    [(dari, 2), (sisi, 1), (kekanakan, 1), (orang,...\n",
      "4                     [(kamu, 1), (cok, 1), (kevr, 1)]\n",
      "Name: Text_tokens_fdist, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# NLTK calc frequency distribution\n",
    "def freqDist_wrapper(text):\n",
    "    return FreqDist(text)\n",
    "\n",
    "data['Text_tokens_fdist'] = data['Text_tokens'].apply(freqDist_wrapper)\n",
    "\n",
    "print('Frequency Tokens : \\n') \n",
    "print(data['Text_tokens_fdist'].head().apply(lambda x : x.most_common()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55cc9846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SastrawiNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
      "Installing collected packages: Sastrawi\n",
      "Successfully installed Sastrawi-1.0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "509da606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Sastrawi package\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07a3d1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489\n",
      "------------------------\n",
      "husnan : husnan\n",
      "filihan : filihan\n",
      "yang : yang\n",
      "terbaik : baik\n",
      "aku : aku\n",
      "jangan : jangan\n",
      "merokok : rokok\n",
      "yaa : yaa\n",
      "mending : mending\n",
      "deketin : deketin\n",
      "aja : aja\n",
      "soalnya : soal\n",
      "kan : kan\n",
      "rokok : rokok\n",
      "membunuhmu : bunuh\n",
      "sedangkan : sedang\n",
      "ya : ya\n",
      "mencintaimu : cinta\n",
      "dari : dari\n",
      "sisi : sisi\n",
      "kekanakan : kana\n",
      "orang : orang\n",
      "dewasa : dewasa\n",
      "bisa : bisa\n",
      "dilihat : lihat\n",
      "supirsupir : supirsupir\n",
      "truk : truk\n",
      "saling : saling\n",
      "klaksonklaksonan : klaksonklaksonan\n",
      "di : di\n",
      "bawah : bawah\n",
      "trowonga : trowonga\n",
      "kamu : kamu\n",
      "cok : cok\n",
      "kevr : kevr\n",
      "siapapun : siapa\n",
      "ngelawak : ngelawak\n",
      "dong : dong\n",
      "pake : pake\n",
      "meme : meme\n",
      "cepat : cepat\n",
      "berikan : ikan\n",
      "yuk : yuk\n",
      "barengbareng : barengbareng\n",
      "selamat : selamat\n",
      "pagi : pagi\n",
      "lupa : lupa\n",
      "sikat : sikat\n",
      "gigi : gigi\n",
      "biar : biar\n",
      "mulut : mulut\n",
      "tidak : tidak\n",
      "bau : bau\n",
      "naga : naga\n",
      "gelap : gelap\n",
      "artis : artis\n",
      "umur : umur\n",
      "berapa : berapa\n",
      "kalian : kalian\n",
      "tau : tau\n",
      "kalau : kalau\n",
      "kepanjangan : panjang\n",
      "gurita : gurita\n",
      "adalah : adalah\n",
      "gurih : gurih\n",
      "tiada : tiada\n",
      "tara : tara\n",
      "nangis : nang\n",
      "bgt : bgt\n",
      "idung : idung\n",
      "kek : kek\n",
      "teknologi : teknologi\n",
      "heh : heh\n",
      "wkwkwk : wkwkwk\n",
      "gue : gue\n",
      "dulu : dulu\n",
      "mamah : mamah\n",
      "tp : tp\n",
      "udah : udah\n",
      "tobat : tobat\n",
      "kok : kok\n",
      "ratarata : ratarata\n",
      "muka : muka\n",
      "kebanyakan : banyak\n",
      "kucing : kucing\n",
      "tu : tu\n",
      "pada : pada\n",
      "photogenic : photogenic\n",
      "aya : aya\n",
      "wae : wae\n",
      "cut : cut\n",
      "off : off\n",
      "orangorang : orangorang\n",
      "toxic : toxic\n",
      "amp : amp\n",
      "memperkecil : kecil\n",
      "circle : circle\n",
      "bersosialisasi : sosial\n",
      "dalam : dalam\n",
      "hidup : hidup\n",
      "kita : kita\n",
      "itu : itu\n",
      "betul : betul\n",
      "sebuah : buah\n",
      "pilihan : pilih\n",
      "tapi : tapi\n",
      "janga : janga\n",
      "seru : seru\n",
      "banget : banget\n",
      "gapake : gapake\n",
      "sempak : sempak\n",
      "mau : mau\n",
      "tepuk : tepuk\n",
      "tangan : tangan\n",
      "buat : buat\n",
      "army : army\n",
      "indonesia : indonesia\n",
      "keren : keren\n",
      "galiat : galiat\n",
      "semuanya : semua\n",
      "nyanyi : nyanyi\n",
      "disanaa : disanaa\n",
      "suka : suka\n",
      "tbtb : tbtb\n",
      "bersenandung : senandung\n",
      "dipaksa : paksa\n",
      "terimakasih : terimakasih\n",
      "medan : medan\n",
      "bantuin : bantuin\n",
      "turut : turut\n",
      "berdukacita : dukacita\n",
      "untuk : untuk\n",
      "korban : korban\n",
      "kerusuhan : rusuh\n",
      "stadion : stadion\n",
      "kanjuruhan : kanjuruhan\n",
      "malang : malang\n",
      "hamil : hamil\n",
      "eek : eek\n",
      "its : its\n",
      "been : been\n",
      "while : while\n",
      "since : since\n",
      "membahas : bahas\n",
      "perpupingan : perpupingan\n",
      "duniawi : duniawi\n",
      "and : and\n",
      "know : know\n",
      "what : what\n",
      "im : im\n",
      "holding : holding\n",
      "back : back\n",
      "to : to\n",
      "defecate : defecate\n",
      "for : for\n",
      "li : li\n",
      "mikha : mikha\n",
      "ka : ka\n",
      "rt : rt\n",
      "juicy : juicy\n",
      "luicy : luicy\n",
      "feat : feat\n",
      "ziva : ziva\n",
      "magnolya : magnolya\n",
      "tampar : tampar\n",
      "live : live\n",
      "performance : performance\n",
      "via : via\n",
      "https : https\n",
      "egois : egois\n",
      "padahal : padahal\n",
      "ngerasa : ngerasa\n",
      "diri : diri\n",
      "sendiri : sendiri\n",
      "eh : eh\n",
      "ternyata : nyata\n",
      "ada : ada\n",
      "lebih : lebih\n",
      "berarti : arti\n",
      "wkwk : wkwk\n",
      "mbamba : mbamba\n",
      "resto : resto\n",
      "mirip : mirip\n",
      "teh : teh\n",
      "jipa : jipa\n",
      "heeh : heeh\n",
      "mak : mak\n",
      "the : the\n",
      "one : one\n",
      "only : only\n",
      "anakku : anak\n",
      "seyeng : seyeng\n",
      "bobo : bobo\n",
      "dikasur : kasur\n",
      "seharian : hari\n",
      "gt : gt\n",
      "nongkrong : nongkrong\n",
      "mas : mas\n",
      "yu : yu\n",
      "nunggu : nunggu\n",
      "bgtbgtbgt : bgtbgtbgt\n",
      "rush : rush\n",
      "hour : hour\n",
      "rilis : rilis\n",
      "plis : plis\n",
      "woi : woi\n",
      "gebetan : gebetan\n",
      "jharapan : jharapan\n",
      "dipercepat : cepat\n",
      "ini : ini\n",
      "penggalan : penggal\n",
      "lirik : lirik\n",
      "lagu : lagu\n",
      "judulnya : judul\n",
      "bahagia : bahagia\n",
      "jadi : jadi\n",
      "lah : lah\n",
      "sambil : sambil\n",
      "joget : joget\n",
      "lagunya : lagu\n",
      "nih : nih\n",
      "satunya : satu\n",
      "kukira : kira\n",
      "akan : akan\n",
      "satu : satu\n",
      "bukan : bukan\n",
      "hanya : hanya\n",
      "salah : salah\n",
      "kalo : kalo\n",
      "seneng : neng\n",
      "bangeettt : bangeettt\n",
      "ngenalin : ngenalin\n",
      "publik : publik\n",
      "terus : terus\n",
      "ngajak : ngajak\n",
      "foto : foto\n",
      "bareng : bareng\n",
      "lagi : lagi\n",
      "masker : masker\n",
      "berpengalaman : alam\n",
      "ye : ye\n",
      "kite : kite\n",
      "emang : emang\n",
      "gitu : gitu\n",
      "sih : sih\n",
      "membentuk : bentuk\n",
      "personal : personal\n",
      "brandingnya : brandingnya\n",
      "kaya : kaya\n",
      "malaikat : malaikat\n",
      "biasanya : biasa\n",
      "aslinya : asli\n",
      "dajal : dajal\n",
      "announcing : announcing\n",
      "wtf : wtf\n",
      "exclusive : exclusive\n",
      "an : an\n",
      "allstar : allstar\n",
      "act : act\n",
      "featuring : featuring\n",
      "tiara : tiara\n",
      "andin : andin\n",
      "parah : parah\n",
      "nonton : nonton\n",
      "tiktok : tiktok\n",
      "barusan : barusan\n",
      "gw : gw\n",
      "upload : upload\n",
      "happy : happy\n",
      "million : million\n",
      "views : views\n",
      "mv : mv\n",
      "on : on\n",
      "youtube : youtube\n",
      "keep : keep\n",
      "streaming : streaming\n",
      "sangat : sangat\n",
      "paham : paham\n",
      "mengerti : erti\n",
      "dan : dan\n",
      "sini : sini\n",
      "peluk : peluk\n",
      "kurang : kurang\n",
      "estetik : estetik\n",
      "pilter : pilter\n",
      "aing : aing\n",
      "kayang : kayang\n",
      "dah : dah\n",
      "sumpah : sumpah\n",
      "kaget : kaget\n",
      "tiba : tiba\n",
      "trending : trending\n",
      "umum : umum\n",
      "teori : teori\n",
      "konspirasi : konspirasi\n",
      "ending : ending\n",
      "team : team\n",
      "mana : mana\n",
      "ngubur : ngubur\n",
      "box : box\n",
      "berisi : isi\n",
      "kenangan : kenang\n",
      "bersama : sama\n",
      "chicco : chicco\n",
      "hab : hab\n",
      "kak : kak\n",
      "pesanannya : pesan\n",
      "hidangkan : hidang\n",
      "selagi : selagi\n",
      "hangat : hangat\n",
      "menikmati : nikmat\n",
      "official : official\n",
      "music : music\n",
      "video : video\n",
      "tautau : tautau\n",
      "menit : menit\n",
      "menuju : tuju\n",
      "jangaan : jangaan\n",
      "dipaksaaa : dipaksaaa\n",
      "haha : haha\n",
      "minyak : minyak\n",
      "goreng : goreng\n",
      "taboook : taboook\n",
      "nii : nii\n",
      "hihiii : hihiii\n",
      "before : before\n",
      "after : after\n",
      "bapak : bapak\n",
      "tiap : tiap\n",
      "hari : hari\n",
      "tersambo : tersambo\n",
      "sambo : sambo\n",
      "fess : fess\n",
      "imut : imut\n",
      "lol : lol\n",
      "iya : iya\n",
      "apa : apa\n",
      "namanyaa : namanyaa\n",
      "taichan : taichan\n",
      "jakarta : jakarta\n",
      "paling : paling\n",
      "enak : enak\n",
      "dimanasi : dimanasi\n",
      "harus : harus\n",
      "bikin : bikin\n",
      "hepi : hepi\n",
      "asli : asli\n",
      "semua : semua\n",
      "dengerin : dengerin\n",
      "bantu : bantu\n",
      "stream : stream\n",
      "spotify : spotify\n",
      "tuh : tuh\n",
      "makasih : makasih\n",
      "yyyy : yyyy\n",
      "ente : ente\n",
      "kadang : kadang\n",
      "berekspektasi : berekspektasi\n",
      "nanti : nanti\n",
      "gak : gak\n",
      "nungguin : nungguin\n",
      "malah : malah\n",
      "masih : masih\n",
      "gr : gr\n",
      "serame : serame\n",
      "sampai : sampai\n",
      "bertemu : temu\n",
      "panggungpanggung : panggungpanggung\n",
      "lainnya : lain\n",
      "ih : ih\n",
      "lucu : lucu\n",
      "cocok : cocok\n",
      "peranin : peranin\n",
      "siapa : siapa\n",
      "gini : gin\n",
      "jir : jir\n",
      "or : or\n",
      "thank : thank\n",
      "you : you\n",
      "so : so\n",
      "much : much\n",
      "semalem : semalem\n",
      "social : social\n",
      "chic : chic\n",
      "jujur : jujur\n",
      "nervous : nervous\n",
      "first : first\n",
      "time : time\n",
      "nya : nya\n",
      "diem : diem\n",
      "aj : aj\n",
      "gasi : gas\n",
      "gpp : gpp\n",
      "ga : ga\n",
      "minta : minta\n",
      "maap : maap\n",
      "ngga : ngga\n",
      "gausah : gausah\n",
      "membenarkan : benar\n",
      "posisi : posisi\n",
      "lu : lu\n",
      "jelasjelas : jelasjelas\n",
      "iiih : iiih\n",
      "anjir : anjir\n",
      "niat : niat\n",
      "film : film\n",
      "nope : nope\n",
      "kemaren : kemaren\n",
      "terakhir : akhir\n",
      "ditayangin : ditayangin\n",
      "bioskopnya : bioskop\n",
      "faaaagg : faaaagg\n",
      "ikan : ikan\n",
      "sama : sama\n",
      "lini : lini\n",
      "gonta : gonta\n",
      "ganti : ganti\n",
      "dapet : dapet\n",
      "salam : salam\n",
      "plot : plot\n",
      "twist : twist\n",
      "pokoke : pokoke\n",
      "surprise : surprise\n",
      "tanggal : tanggal\n",
      "hehe : hehe\n",
      "ill : ill\n",
      "see : see\n",
      "guys : guys\n",
      "there : there\n",
      "ayo : ayo\n",
      "bila : bila\n",
      "tlah : tlah\n",
      "berbeda : beda\n",
      "abis : abis\n",
      "yura : yura\n",
      "boleh : boleh\n",
      "juga : juga\n",
      "ehe : ehe\n",
      "is : is\n",
      "half : half\n",
      "of : of\n",
      "my : my\n",
      "lyfe : lyfe\n",
      "thanks : thanks\n",
      "this : this\n",
      "gorgeous : gorgeous\n",
      "person : person\n",
      "cita : cita\n",
      "jaman : jaman\n",
      "tsm : tsm\n",
      "pengen : ken\n",
      "top : top\n",
      "tracks : tracks\n",
      "receiptify : receiptify\n",
      "fullin : fullin\n",
      "njip : njip\n",
      "find : find\n",
      "courage : courage\n",
      "leave : leave\n",
      "table : table\n",
      "if : if\n",
      "respect : respect\n",
      "no : no\n",
      "longer : longer\n",
      "being : being\n",
      "served : served\n",
      "cipung : cipung\n",
      "rafathar : rafathar\n",
      "pipis : pipis\n",
      "ngetweet : ngetweet\n",
      "pas : pas\n",
      "kerja : kerja\n",
      "ngantuk : ngantuk\n",
      "rebahan : rebah\n",
      "ngantuknya : ngantuknya\n",
      "hilang : hilang\n",
      "berasa : asa\n",
      "diprank : diprank\n",
      "badan : badan\n",
      "bgd : bgd\n",
      "sm : sm\n",
      "poni : poni\n",
      "kecombrang : kecombrang\n",
      "sekali : sekali\n",
      "amin : amin\n",
      "nanggung : nanggung\n",
      "gaksiee : gaksiee\n",
      "mulu : mulu\n",
      "hahaha : hahaha\n",
      "{'husnan': 'husnan', 'filihan': 'filihan', 'yang': 'yang', 'terbaik': 'baik', 'aku': 'aku', 'jangan': 'jangan', 'merokok': 'rokok', 'yaa': 'yaa', 'mending': 'mending', 'deketin': 'deketin', 'aja': 'aja', 'soalnya': 'soal', 'kan': 'kan', 'rokok': 'rokok', 'membunuhmu': 'bunuh', 'sedangkan': 'sedang', 'ya': 'ya', 'mencintaimu': 'cinta', 'dari': 'dari', 'sisi': 'sisi', 'kekanakan': 'kana', 'orang': 'orang', 'dewasa': 'dewasa', 'bisa': 'bisa', 'dilihat': 'lihat', 'supirsupir': 'supirsupir', 'truk': 'truk', 'saling': 'saling', 'klaksonklaksonan': 'klaksonklaksonan', 'di': 'di', 'bawah': 'bawah', 'trowonga': 'trowonga', 'kamu': 'kamu', 'cok': 'cok', 'kevr': 'kevr', 'siapapun': 'siapa', 'ngelawak': 'ngelawak', 'dong': 'dong', 'pake': 'pake', 'meme': 'meme', 'cepat': 'cepat', 'berikan': 'ikan', 'yuk': 'yuk', 'barengbareng': 'barengbareng', 'selamat': 'selamat', 'pagi': 'pagi', 'lupa': 'lupa', 'sikat': 'sikat', 'gigi': 'gigi', 'biar': 'biar', 'mulut': 'mulut', 'tidak': 'tidak', 'bau': 'bau', 'naga': 'naga', 'gelap': 'gelap', 'artis': 'artis', 'umur': 'umur', 'berapa': 'berapa', 'kalian': 'kalian', 'tau': 'tau', 'kalau': 'kalau', 'kepanjangan': 'panjang', 'gurita': 'gurita', 'adalah': 'adalah', 'gurih': 'gurih', 'tiada': 'tiada', 'tara': 'tara', 'nangis': 'nang', 'bgt': 'bgt', 'idung': 'idung', 'kek': 'kek', 'teknologi': 'teknologi', 'heh': 'heh', 'wkwkwk': 'wkwkwk', 'gue': 'gue', 'dulu': 'dulu', 'mamah': 'mamah', 'tp': 'tp', 'udah': 'udah', 'tobat': 'tobat', 'kok': 'kok', 'ratarata': 'ratarata', 'muka': 'muka', 'kebanyakan': 'banyak', 'kucing': 'kucing', 'tu': 'tu', 'pada': 'pada', 'photogenic': 'photogenic', 'aya': 'aya', 'wae': 'wae', 'cut': 'cut', 'off': 'off', 'orangorang': 'orangorang', 'toxic': 'toxic', 'amp': 'amp', 'memperkecil': 'kecil', 'circle': 'circle', 'bersosialisasi': 'sosial', 'dalam': 'dalam', 'hidup': 'hidup', 'kita': 'kita', 'itu': 'itu', 'betul': 'betul', 'sebuah': 'buah', 'pilihan': 'pilih', 'tapi': 'tapi', 'janga': 'janga', 'seru': 'seru', 'banget': 'banget', 'gapake': 'gapake', 'sempak': 'sempak', 'mau': 'mau', 'tepuk': 'tepuk', 'tangan': 'tangan', 'buat': 'buat', 'army': 'army', 'indonesia': 'indonesia', 'keren': 'keren', 'galiat': 'galiat', 'semuanya': 'semua', 'nyanyi': 'nyanyi', 'disanaa': 'disanaa', 'suka': 'suka', 'tbtb': 'tbtb', 'bersenandung': 'senandung', 'dipaksa': 'paksa', 'terimakasih': 'terimakasih', 'medan': 'medan', 'bantuin': 'bantuin', 'turut': 'turut', 'berdukacita': 'dukacita', 'untuk': 'untuk', 'korban': 'korban', 'kerusuhan': 'rusuh', 'stadion': 'stadion', 'kanjuruhan': 'kanjuruhan', 'malang': 'malang', 'hamil': 'hamil', 'eek': 'eek', 'its': 'its', 'been': 'been', 'while': 'while', 'since': 'since', 'membahas': 'bahas', 'perpupingan': 'perpupingan', 'duniawi': 'duniawi', 'and': 'and', 'know': 'know', 'what': 'what', 'im': 'im', 'holding': 'holding', 'back': 'back', 'to': 'to', 'defecate': 'defecate', 'for': 'for', 'li': 'li', 'mikha': 'mikha', 'ka': 'ka', 'rt': 'rt', 'juicy': 'juicy', 'luicy': 'luicy', 'feat': 'feat', 'ziva': 'ziva', 'magnolya': 'magnolya', 'tampar': 'tampar', 'live': 'live', 'performance': 'performance', 'via': 'via', 'https': 'https', 'egois': 'egois', 'padahal': 'padahal', 'ngerasa': 'ngerasa', 'diri': 'diri', 'sendiri': 'sendiri', 'eh': 'eh', 'ternyata': 'nyata', 'ada': 'ada', 'lebih': 'lebih', 'berarti': 'arti', 'wkwk': 'wkwk', 'mbamba': 'mbamba', 'resto': 'resto', 'mirip': 'mirip', 'teh': 'teh', 'jipa': 'jipa', 'heeh': 'heeh', 'mak': 'mak', 'the': 'the', 'one': 'one', 'only': 'only', 'anakku': 'anak', 'seyeng': 'seyeng', 'bobo': 'bobo', 'dikasur': 'kasur', 'seharian': 'hari', 'gt': 'gt', 'nongkrong': 'nongkrong', 'mas': 'mas', 'yu': 'yu', 'nunggu': 'nunggu', 'bgtbgtbgt': 'bgtbgtbgt', 'rush': 'rush', 'hour': 'hour', 'rilis': 'rilis', 'plis': 'plis', 'woi': 'woi', 'gebetan': 'gebetan', 'jharapan': 'jharapan', 'dipercepat': 'cepat', 'ini': 'ini', 'penggalan': 'penggal', 'lirik': 'lirik', 'lagu': 'lagu', 'judulnya': 'judul', 'bahagia': 'bahagia', 'jadi': 'jadi', 'lah': 'lah', 'sambil': 'sambil', 'joget': 'joget', 'lagunya': 'lagu', 'nih': 'nih', 'satunya': 'satu', 'kukira': 'kira', 'akan': 'akan', 'satu': 'satu', 'bukan': 'bukan', 'hanya': 'hanya', 'salah': 'salah', 'kalo': 'kalo', 'seneng': 'neng', 'bangeettt': 'bangeettt', 'ngenalin': 'ngenalin', 'publik': 'publik', 'terus': 'terus', 'ngajak': 'ngajak', 'foto': 'foto', 'bareng': 'bareng', 'lagi': 'lagi', 'masker': 'masker', 'berpengalaman': 'alam', 'ye': 'ye', 'kite': 'kite', 'emang': 'emang', 'gitu': 'gitu', 'sih': 'sih', 'membentuk': 'bentuk', 'personal': 'personal', 'brandingnya': 'brandingnya', 'kaya': 'kaya', 'malaikat': 'malaikat', 'biasanya': 'biasa', 'aslinya': 'asli', 'dajal': 'dajal', 'announcing': 'announcing', 'wtf': 'wtf', 'exclusive': 'exclusive', 'an': 'an', 'allstar': 'allstar', 'act': 'act', 'featuring': 'featuring', 'tiara': 'tiara', 'andin': 'andin', 'parah': 'parah', 'nonton': 'nonton', 'tiktok': 'tiktok', 'barusan': 'barusan', 'gw': 'gw', 'upload': 'upload', 'happy': 'happy', 'million': 'million', 'views': 'views', 'mv': 'mv', 'on': 'on', 'youtube': 'youtube', 'keep': 'keep', 'streaming': 'streaming', 'sangat': 'sangat', 'paham': 'paham', 'mengerti': 'erti', 'dan': 'dan', 'sini': 'sini', 'peluk': 'peluk', 'kurang': 'kurang', 'estetik': 'estetik', 'pilter': 'pilter', 'aing': 'aing', 'kayang': 'kayang', 'dah': 'dah', 'sumpah': 'sumpah', 'kaget': 'kaget', 'tiba': 'tiba', 'trending': 'trending', 'umum': 'umum', 'teori': 'teori', 'konspirasi': 'konspirasi', 'ending': 'ending', 'team': 'team', 'mana': 'mana', 'ngubur': 'ngubur', 'box': 'box', 'berisi': 'isi', 'kenangan': 'kenang', 'bersama': 'sama', 'chicco': 'chicco', 'hab': 'hab', 'kak': 'kak', 'pesanannya': 'pesan', 'hidangkan': 'hidang', 'selagi': 'selagi', 'hangat': 'hangat', 'menikmati': 'nikmat', 'official': 'official', 'music': 'music', 'video': 'video', 'tautau': 'tautau', 'menit': 'menit', 'menuju': 'tuju', 'jangaan': 'jangaan', 'dipaksaaa': 'dipaksaaa', 'haha': 'haha', 'minyak': 'minyak', 'goreng': 'goreng', 'taboook': 'taboook', 'nii': 'nii', 'hihiii': 'hihiii', 'before': 'before', 'after': 'after', 'bapak': 'bapak', 'tiap': 'tiap', 'hari': 'hari', 'tersambo': 'tersambo', 'sambo': 'sambo', 'fess': 'fess', 'imut': 'imut', 'lol': 'lol', 'iya': 'iya', 'apa': 'apa', 'namanyaa': 'namanyaa', 'taichan': 'taichan', 'jakarta': 'jakarta', 'paling': 'paling', 'enak': 'enak', 'dimanasi': 'dimanasi', 'harus': 'harus', 'bikin': 'bikin', 'hepi': 'hepi', 'asli': 'asli', 'semua': 'semua', 'dengerin': 'dengerin', 'bantu': 'bantu', 'stream': 'stream', 'spotify': 'spotify', 'tuh': 'tuh', 'makasih': 'makasih', 'yyyy': 'yyyy', 'ente': 'ente', 'kadang': 'kadang', 'berekspektasi': 'berekspektasi', 'nanti': 'nanti', 'gak': 'gak', 'nungguin': 'nungguin', 'malah': 'malah', 'masih': 'masih', 'gr': 'gr', 'serame': 'serame', 'sampai': 'sampai', 'bertemu': 'temu', 'panggungpanggung': 'panggungpanggung', 'lainnya': 'lain', 'ih': 'ih', 'lucu': 'lucu', 'cocok': 'cocok', 'peranin': 'peranin', 'siapa': 'siapa', 'gini': 'gin', 'jir': 'jir', 'or': 'or', 'thank': 'thank', 'you': 'you', 'so': 'so', 'much': 'much', 'semalem': 'semalem', 'social': 'social', 'chic': 'chic', 'jujur': 'jujur', 'nervous': 'nervous', 'first': 'first', 'time': 'time', 'nya': 'nya', 'diem': 'diem', 'aj': 'aj', 'gasi': 'gas', 'gpp': 'gpp', 'ga': 'ga', 'minta': 'minta', 'maap': 'maap', 'ngga': 'ngga', 'gausah': 'gausah', 'membenarkan': 'benar', 'posisi': 'posisi', 'lu': 'lu', 'jelasjelas': 'jelasjelas', 'iiih': 'iiih', 'anjir': 'anjir', 'niat': 'niat', 'film': 'film', 'nope': 'nope', 'kemaren': 'kemaren', 'terakhir': 'akhir', 'ditayangin': 'ditayangin', 'bioskopnya': 'bioskop', 'faaaagg': 'faaaagg', 'ikan': 'ikan', 'sama': 'sama', 'lini': 'lini', 'gonta': 'gonta', 'ganti': 'ganti', 'dapet': 'dapet', 'salam': 'salam', 'plot': 'plot', 'twist': 'twist', 'pokoke': 'pokoke', 'surprise': 'surprise', 'tanggal': 'tanggal', 'hehe': 'hehe', 'ill': 'ill', 'see': 'see', 'guys': 'guys', 'there': 'there', 'ayo': 'ayo', 'bila': 'bila', 'tlah': 'tlah', 'berbeda': 'beda', 'abis': 'abis', 'yura': 'yura', 'boleh': 'boleh', 'juga': 'juga', 'ehe': 'ehe', 'is': 'is', 'half': 'half', 'of': 'of', 'my': 'my', 'lyfe': 'lyfe', 'thanks': 'thanks', 'this': 'this', 'gorgeous': 'gorgeous', 'person': 'person', 'cita': 'cita', 'jaman': 'jaman', 'tsm': 'tsm', 'pengen': 'ken', 'top': 'top', 'tracks': 'tracks', 'receiptify': 'receiptify', 'fullin': 'fullin', 'njip': 'njip', 'find': 'find', 'courage': 'courage', 'leave': 'leave', 'table': 'table', 'if': 'if', 'respect': 'respect', 'no': 'no', 'longer': 'longer', 'being': 'being', 'served': 'served', 'cipung': 'cipung', 'rafathar': 'rafathar', 'pipis': 'pipis', 'ngetweet': 'ngetweet', 'pas': 'pas', 'kerja': 'kerja', 'ngantuk': 'ngantuk', 'rebahan': 'rebah', 'ngantuknya': 'ngantuknya', 'hilang': 'hilang', 'berasa': 'asa', 'diprank': 'diprank', 'badan': 'badan', 'bgd': 'bgd', 'sm': 'sm', 'poni': 'poni', 'kecombrang': 'kecombrang', 'sekali': 'sekali', 'amin': 'amin', 'nanggung': 'nanggung', 'gaksiee': 'gaksiee', 'mulu': 'mulu', 'hahaha': 'hahaha'}\n",
      "------------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Text_tokens_WSW'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Text_tokens_WSW'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_stemmed_term\u001b[39m(document):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [term_dict[term] \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m document]\n\u001b[1;32m---> 31\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText_tokens_stemmed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mText_tokens_WSW\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mswifter\u001b[38;5;241m.\u001b[39mapply(get_stemmed_term)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText_tokens_stemmed\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Text_tokens_WSW'"
     ]
    }
   ],
   "source": [
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "term_dict = {}\n",
    "\n",
    "for document in data['Text_tokens_fdist']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "            \n",
    "print(len(term_dict))\n",
    "print(\"------------------------\")\n",
    "\n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "    print(term,\":\" ,term_dict[term])\n",
    "    \n",
    "print(term_dict)\n",
    "print(\"------------------------\")\n",
    "\n",
    "\n",
    "# apply stemmed term to dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "data['Text_tokens_stemmed'] = data['Text_tokens_WSW'].swifter.apply(get_stemmed_term)\n",
    "print(data['Text_tokens_stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3372b44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Text_tokens</th>\n",
       "      <th>Text_tokens_fdist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>husnan</td>\n",
       "      <td>[husnan]</td>\n",
       "      <td>{'husnan': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>filihan yang terbaik</td>\n",
       "      <td>[filihan, yang, terbaik]</td>\n",
       "      <td>{'filihan': 1, 'yang': 1, 'terbaik': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jangan merokok yaa mending deketin aku aja soa...</td>\n",
       "      <td>[jangan, merokok, yaa, mending, deketin, aku, ...</td>\n",
       "      <td>{'jangan': 1, 'merokok': 1, 'yaa': 1, 'mending...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sisi kekanakan dari orang dewasa bisa dilihat ...</td>\n",
       "      <td>[sisi, kekanakan, dari, orang, dewasa, bisa, d...</td>\n",
       "      <td>{'sisi': 1, 'kekanakan': 1, 'dari': 2, 'orang'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kamu cok kevr</td>\n",
       "      <td>[kamu, cok, kevr]</td>\n",
       "      <td>{'kamu': 1, 'cok': 1, 'kevr': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  \\\n",
       "0                                             husnan   \n",
       "1                               filihan yang terbaik   \n",
       "2  jangan merokok yaa mending deketin aku aja soa...   \n",
       "3  sisi kekanakan dari orang dewasa bisa dilihat ...   \n",
       "4                                      kamu cok kevr   \n",
       "\n",
       "                                         Text_tokens  \\\n",
       "0                                           [husnan]   \n",
       "1                           [filihan, yang, terbaik]   \n",
       "2  [jangan, merokok, yaa, mending, deketin, aku, ...   \n",
       "3  [sisi, kekanakan, dari, orang, dewasa, bisa, d...   \n",
       "4                                  [kamu, cok, kevr]   \n",
       "\n",
       "                                   Text_tokens_fdist  \n",
       "0                                      {'husnan': 1}  \n",
       "1            {'filihan': 1, 'yang': 1, 'terbaik': 1}  \n",
       "2  {'jangan': 1, 'merokok': 1, 'yaa': 1, 'mending...  \n",
       "3  {'sisi': 1, 'kekanakan': 1, 'dari': 2, 'orang'...  \n",
       "4                   {'kamu': 1, 'cok': 1, 'kevr': 1}  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50fb8a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('dataset_ziva.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f280a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
